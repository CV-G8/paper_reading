# Dimensionality Reduction by Learning an Invariant Mapping

---

R. Hadsell, S. Chopra, Y. Lecun, [Dimensionality Reduction by Learning an Invariant Mapping][invariant_mapping], CVPR (2006)

[invariant_mapping]: http://www.cs.toronto.edu/~hinton/csc2535/readings/hadsell-chopra-lecun-06-1.pdf "Dimensionality Reduction by Learning an Invariant Mapping"

---

## 摘要

降维（dimensionality reduction）：将高维数据点映射到低维流形上，使输入空间中相似的点在流形上相距较近（mapping a set of high dimensional input points onto a low dimensional manifold so that “similar” points in input space are mapped to nearby points on the manifold）。

已知方法的缺点：（1）与输入空间中的距离测度相关（most of them depend on a meaningful and computable distance metric in input space）；（2）当新样本与训练数据关系未知时，没有能够准确映射该样本的函数（do not compute a “function” that can accurately map new input samples whose relationship to the training data is unknown）。

本文提出通过学习不变映射进行降维（Dimensionality Reduction by Learning an Invariant Mapping，DrLIM），即学习一个能够将数据均匀映射到输出流形上的全局一致非线性函数（learning a globally coherent non-linear function that maps the data evenly to the output manifold），该学习仅与近邻关系有关而无需输入空间中任何距离度量（the learning relies solely on neighborhood relationships and does not require any distance measure in the input space）。

## 1 引言

局部线性嵌入（Locally Linear Embedding，LLE）：对类别相同的输入向量线性组合，无法处理与训练样本关系未知的数据。

样本外扩展（out-of-sample extensions）：假设存在能够生成邻域矩阵的可计算核函数（assume the existence of a computable kernel function that is used to generate the neighborhood matrix）条件下，给出新样本的一致性嵌入（consistent embedding）。

此外，在输出空间中，上述方法容易使样本聚集过密而导致解退化（degenerate solutions）；相反，这些方法需要找到能够被样本均匀覆盖的流形。

学习不变映射进行降维（Dimensionality Reduction by Learning an Invariant Mapping，DrLIM）通过学习全局一致非线性函数，数据映射到输出流形上：

* 仅需训练样本间的相邻关系（neighborhood relationships between training samples）；

* 对输入的非线性变换映射不变（invariant to complicated non-linear trnasformations of the inputs such as lighting changes and geometric distortions）；

* 无先验条件下，映射未知新样本（map new samples not seen during training, with no prior knowledge）；

* 输出空间上映射平滑、一致（mapping generated by the function is in some sense “smooth” and coherent in the output space）。

对比损失函数

### 1.1 相关工作（Previous Work）

Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS)

## 2 学习低维映射（Learning the Low Dimensional Mapping）

给定输入空间中样本间相邻关系（neighborhood relationships between samples）


### 2.1 对比损失函数（The Contrastive Loss Function）

### 2.2 （Spring Model Analogy）

### 2.3 算法（The Algorithm）

## 3 实验

### 3.1 训练体系（Training Architecture）

### 3.2 MNIST样本映射

### 3.3 MNIST样本平移不变映射

## 4 讨论
